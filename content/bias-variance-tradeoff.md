+++
Categories = ["Computation", "Learning"]
bibfile = "ccnlab.json"
+++

The **bias-variance tradeoff**, also known as the "no free lunch" principle, was articulated by statisticians [[@^GemanGeman84]] and [[@^VapnikChervonenkis71]] among others. It provides essential understanding of how much data different types of learning systems need in order to exhibit [[generalization]] to novel inputs, and how hard-wired _biases_ (e.g., from evolution-shaped neural properties) can allow a system to learn from less data, but not without costs.

_If_ these biases are appropriate for the actual world in which the learning system is being trained on, then clearly they will be beneficial. For example, building in a bias to favor highly nutritious foods, and even what those foods look like in their natural environment, will allow a biological organism to survive better than one without such biases. However, the "no free lunch" part of the tradeoff is that there are no _universally beneficial biases_: every bias you introduce is beneficial for some worlds, but harmful for other possible worlds (e.g., where the nutritious foods look different, or are actually poisonous; there goes your lunch...).

The _variance_ aspect of the tradeoff provides a way of capturing the consequences of weakly biased learning systems: if you have weak biases, there will be correspondingly greater variance in what these systems learn given a fixed amount of training data. This variance will be manifest for example in the behavior of the system when confronted with novel testing data: each instance of a weakly-biased model will behave in a very different way (i.e., high variance), relative to a more strongly biased model, which will exhibit less bias and essentially always behave according to its biases.

One important implication of this tradeoff is that, if you do happen to have a large amount of data, it is generally better to use weakly biased, general-purpose learning systems, because the biases are unnecessary and can only get in your way (no free lunch). This _big data_ mantra is articulated in Rich Sutton's [bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) for example. Existing [[abstract neural network]] (ANN) models have shown that this big data approach is very powerful, provided you can find a way to leverage large amounts of data, as in [[large language models]] (LLMs) trained on practically everything that humans have ever written.

The key to making the big data approach work is to use a gradient-based learning algorithm, which effectively performs _parallel_ [[search]] through the data space. Earlier symbolic [[artificial intelligence]] (AI) models can only perform serial search, which quickly succumbs to the [[curse of dimensionality]] as the problem space gets larger. 

Interestingly, the trial-and-error learning problem of [[reinforcement learning]] (RL) presents a unique challenge for the big data approach, because, by definition, it requires a serial search process through the problem space as the individual agent takes actions one-by-one in an environment. Although big data approaches to RL use many replicated agents working in parallel, the curse of dimensionality for the inevitably serial search process of each agent eventually overwhelms the capacity of multi-agent parallelism, and thus the scaling properties of RL models remain well below what is accomplished by LLMs.

This represents a critical challenge for individual biological organisms in the real world, who have to learn in a serial RL-like manner, and must confront the considerable complexity of the natural world.

## Neural biases

How does the bias-variance tradeoff play out in the biological domain that we are focused on here? Is it irrelevant, because biological organisms are only concerned with _one_ world: the actual one in which they must survive? Moreover, the process of evolution provides a ready mechanism for building in all manner of biases that are specifically adaptive for this "real world" environment.

Indeed, evolution can be considered a very general purpose "outer loop" adaptive [[search]] mechanism that operates on a _huge_ amount of data, in the form of all the life-and-death struggles by generations of organisms across the globe, across the 3.5 odd billion or so years of life on earth. Each individual organism is just a brief inner-loop within the scope of this outer-loop of evolutionary search.

Within the brief lifespan of an individual organism, data is a relatively precious commodity, because most organisms need to be able to fend for themselves relatively soon after birth. Indeed, many species come out of the egg or womb with impressive _precocious_ abilities, like the ability to locomote in complex natural environments. Across species, there is a spectrum of the duration of the neonatal learning period after birth known as _neoteny_ ([[@Montagu55]]; [[@Gould77]]), which effectively determines how much an individual animal learns in their own _ontogeny_ (lifetime).

Humans lie at the extreme end of the neoteny spectrum, and are almost absurdly lacking in early precocial abilities relative to other species: we come out of the womb with virtually no skills whatsoever, and it takes somewhere around 2-3 years to even learn to control our bowels properly! Thus, it seems that the human case may be more aligned with the general purpose learning approach. Nevertheless, when you consider all that people can learn to do within a couple of years (aside from bowel control), including learning to speak and toddle around, relative to the amount of actual life experience (data) over that time frame, it is likely that we do have some strong pre-wired biases, along with powerful general-purpose learning mechanisms.

From a neuroscience perspective, the [[neocortex]] has properties consistent with a more weakly-biased, general-purpose learning system, while subcortical brain networks are more strongly shaped by evolution. For example, the [[colliculus]] plays a dominant role in shaping the behavior in many species, and has broad sensory-motor connectivity ([[@HoyFarrow25]]). Likewise, extensive networks through the [[basal ganglia]] drive adaptive motor behavior with a combination of learned and instinctive motor programs.

The subcortical networks work synergistically with the neocortex in humans to shape our behavior and learning, consistent with a system that has both strong biases _and_ powerful general-purpose learning abilities: i.e., the best of both worlds from the bias-variance tradeoff perspective. Furthermore, there is an overall [[subsumption]] dynamic between the neocortex and subcortical systems ([[@Brooks86]]), where the neocortex can override these lower-level systems with its more flexible learned behaviors. The developmental trajectory reflects this transition from subcortical to cortical control ([[@MortonJohnson91]]).

The [[Rubicon]] framework provides a specific set of hypotheses about the nature of the strong pre-wired biases that we get from evolution, as a way of [[computational-cognitive-neuroscience#reverse engineering the brain]], as compared to directly using [[genetic algorithm]]s, which are much less efficient, assuming we can get reasonably far with the reverse-engineering approach. According to this framework, these biases drive goal-driven behavior according to evolutionarily-specified basic drives and motivations, but in so doing, they also allow us to learn novel plans and strategies for adaptive behavior and cognition, so the overall story is an intricate interplay between nature (bias) and nurture (data).


