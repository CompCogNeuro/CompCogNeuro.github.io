+++
Categories = ["Learning", "Computation"]
bibfile = "ccnlab.json"
+++



TODO: goals for this page: 

* hub of links and comparison with Axon, including main page on [[transformers]], which just covers the basic mechanisms but not the results of training on huge amounts of language data.

* future attempt to consolidate lessons learned from LLMS about fundamental issues in representation and dynamics in neural models required to exhibit systematic knowledge -- situate in context of systematicity in the brain paper: [[@^OReillyRanganathRussin22]].  Abstraction, combinatorial dynamics, shortcuts, role of key-value attention heads, etc.


[[combinatorial vs conjunctive]] analyzes LLM in context of overall principles of learning.
