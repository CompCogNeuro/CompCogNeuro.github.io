+++
Categories = ["Learning", "Rubicon"]
bibfile = "ccnlab.json"
+++

**Axon** is an attempt to build a biologically accurate, yet still abstracted and computationally tractable model of neural computation in the mammalian brain (e.g., in rodents, monkeys, and people). The goal is to commit as few _errors of commission_ as possible: the features of the model should not violate any known, well-established properties of the brain. There is still plenty of room for _errors of omission_: not every detail can or should be included.

The approach is guided by an in-depth consideration of the available [[neuroscience]] literature, as well as a strong consideration for the functional and [[computation]]al [[levels of analysis]], informed by an understanding of the known [[cognitive]] and behavioral capabilities of the respective organisms. In other words, it is [[Computational Cognitive Neuroscience]], leveraging notable [[synergies]] across these different levels of analysis. These synergies provide some measure of confidence that the overall approach is on the right track toward answering the fundamental question: _how do the detailed biological mechanisms of the brain give rise to the computationally sophisticated cognitive abilities of the various mammalian species, including humans_?

Given the considerable scope of neuroscience, cognition, and computation covered here, all from coherent, self-consistent perspective, this material is suitable as an introductory textbook, provided here: [[CCN Intro]]. This textbook treatment builds on a series of prior such [[book|textbooks]] going back to [[@^OReillyMunakata00]], which were based on the [[Leabra]] framework that Axon is the direct successor to.

Axon uses more realistic discrete spiking dynamics and associated learning mechanisms relative to Leabra, and supports more advanced brain area models to implement a functional version of the [[Rubicon]] goal-driven learning system, based on the interactions of a large number of [[limbic system|limbic]] brain areas. This motivational system is critical for enabling autonomous learning of cognitive and behavioral skills in an ecologically-realistic manner.

A fundamental challenge for any theoretical understanding of something as complex as the brain, is that we suffer from significant _confirmation bias_ that causes us to contort the scientific data into a form that is consistent with our models, but is not actually how it works. Indeed, the [[bidirectional connectivity]] of the brain that is so central to our models contributes significantly to this phenomenon: it enables us to see what we want to see, not what is actually there. Indeed, we will levy this critique against several other alternative hypotheses along the way (e.g., see [[Hebbian learning]]).

In this context, a primary goal in generating the extensive content here on [compcogneuro.org](https://compcogneuro.org) is to document the full extent to which the current version of Axon captures the existing scientific findings in neuroscience and cognitive psychology, so that the interested reader may form their own opinion about the extent to which the model provides an accurate picture of brain function. Feedback on any and all such issues is encouraged, using the [github discussions](https://github.com/compcogneuro/web/discussions) forum. The [github issues](https://github.com/compcogneuro/web/issues) can be used to report typos or other such "bugs", and pull requests for suggested fixes or other contributions are always welcome, and provide a way to document contributions.

Ultimately, the definitive step in the scientific method is through direct empirical tests of the specific predictions from the model, of which there have been a large number over the years, as documented in the relevant places herein. Perhaps the most central such test is reported in [[Jiang et al 2025]], which directly tests the [[temporal derivative]] form of [[synaptic plasticity]] that drives learning in the Axon model.

## Axon mechanisms

These are the central elements of Axon in terms of neural and computational mechanisms, most of which are well established properties of the mammalian [[neocortex]]:

* [[neuron|Spiking neurons]] with relatively long-lasting [[neuron channels#NMDA]] and [[neuron channels#GABA-B]] channels that support [[stable activation]] states over the course of a roughly 200 msec [[theta cycle]], which is essential for establishing a coherent representation of the current input state. This stability is necessary to drive effective learning

    The Axon neuron model is fairly conventional from a computational neuroscience perspective, featuring two compartments (soma and [[neuron dendrites|dendrite]]) using a range of conductance-based electrophysiologically-accurate [[neuron channels|channels]] and the widely-used _AdEx_ adaptive exponential approximation to the full Hodgkin-Huxley spiking dynamics ([[@BretteGerstner05]]).

    The discrete spiking behavior of these neurons enables effective graded information integration over time in a way that continuous [[rate code activation]] communication does not, by allowing many different signals to be communicated over time, competing for the overall control of the network activation state as a function of the collective integration of spikes within the neurons in the network. As a result, Axon models are overall much more robust and well-behaved overall compared to their [[Leabra]] rate-code based counterparts.

* [[Error-driven learning]] based on errors computed via a [[temporal derivative]] that naturally supports [[predictive learning]], as the difference over time of network activity states representing the prediction followed by the outcome. Local [[synaptic plasticity]] based on the competition between kinases updating at different rates, i.e., the [[kinase algorithm]], naturally computes the error gradient via the temporal derivative dynamic. The result is a fully biologically plausible form of the computationally powerful [[error backpropagation]] algorithm, as shown by the [[GeneRec]] algorithm. Initial empirical support for this mechanism is reported in [[Jiang et al 2025]], in electrophysiological measurements of synaptic plasticity in a rodent preparation.

    The combination of robust error-driven learning and biologically-detailed spiking neurons in Axon enables these neurons to learn to perform arbitrary computational and cognitive tasks. Furthermore, the availability of a clear computational measure of performance in terms of overall learning capability across a wide range of tasks has enabled the optimization of all the biological parameters to maximize learning performance. There is a consistent set of such parameters that generally works best across all the tasks investigated to date, and thus the additional degrees of freedom associated with these parameters are generally eliminated from consideration in constructing new models, greatly reducing the effective complexity of the model.

* [[Bidirectional connectivity]] among excitatory neurons, which is necessary for propagating error signals throughout the network, and pooled [[inhibition]] which is necessary for controlling the effects of bidirectional excitatory connectivity, while also having beneficial computational effects in terms of [[attention]] and competition. Bidirectional connectivity also supports [[constraint satisfaction]] dynamics that can efficiently [[search]] through large high-dimensional knowledge spaces to find (and synthesize) the most relevant information given the current bottom-up (sensory) and top-down (goals) constraints. This results in the use of [[optimized representations]] at each step of processing, which has significant computational advantages.

    Perhaps most importantly, this bidirectional connectivity is widely thought to be essential for [[conscious awareness]] ([[@Lamme06]]), which is likely critical for the system to access its own internal state of knowledge. This ability is notably absent in current [[abstract neural network]] models that drive the widely-used [[large language models]] (LLMs) for example, which are notorious for their inability to accurately evaluate their own knowledge states, resulting in significant _confabulation_. Most experts do not think these models are conscious, which is consistent with the fact that they are based exclusively on feedforward connectivity.

    The central role of bidirectional connectivity in Axon represents one of the most important points of divergence relative to the vast majority of existing neural network models (along with the combination of biologically-detailed spiking dynamics and error-driven learning), and testing the functional importance of this property is a major overarching goal of this research.

These basic neural mechanisms are sufficient to learn well-established functions of the posterior [[neocortex]], including spatially invariant [[object recognition]], spatial processing, and generally developing powerful internal predictive models of the world. See the discussion on [[combinatorial vs. conjunctive]] representations for a summary of the computational-level properties of these learned representations, and how they support effective [[generalization]] to novel situations.

## General intelligence and motivation

If Axon truly captures the essential computational capabilities of the human neocortex, then in principle it should be able to provide a novel and powerful framework for [[artificial intelligence#artificial general intelligence]] (AGI), because humans remain the only undisputed entities capable of fully general intelligence. However, based on what we know about the cognitive neuroscience of human intelligence, it is clear that many other brain areas with specialized functions relative to the posterior neocortex play critical roles as part of a larger overall set of interacting systems required to support our general intelligence abilities.

Computationally, these systems support the functionality associated with [[reinforcement learning#model-based]] reinforcement learning, which is necessary for organizing behavior over time to accomplish goals. Without these mechanisms, the neocortex alone would be a largely passive learning and perceiving system without the ability to organize behavior effectively over time (basically what you experience while dreaming, when the [[prefrontal cortex]] is selectively deactivated).

Thus, the current focus is to understand the basic functionality of these other brain systems, starting with rodents, which have clear homologs to corresponding areas in the primate brain, but everything is much smaller and simpler. Although nobody has accused rodents of having fully general intelligence, they nevertheless have significant survival abilities that depend on a kind of "street smarts", and we believe this is a critical element of human general intelligence as well. Considering what it actually takes for people to solve challenging problems, it seems clear that AGI requires more than just raw generalization: it also depends critically on motivational systems to drive cognition toward solutions, and to focus efforts on the most important problems (i.e., "grit"; [[@Dweck08]]).

The central hypothesis is that there is a core of motivation-related brain areas in the [[prefrontal cortex]], [[basal ganglia]], [[amygdala]], and other areas classically known as the [[limbic system]], which support a powerful and flexible capacity for goal-driven planning and problem solving in rodents (and occupy about 1/3 of the total brain mass, indicating the importance of these areas). This goal-driven system must be sensitive to the overall costs and benefits of actions, in order to commit time and effort to a consistent plan of behavior to accomplish a given goal. 

The Axon model of this system is called [[Rubicon]], based on the evidence that goal engagement produces a major change in the operative _value function_ governing behavior, which serves to maintain goal focus and drives strongly valenced disappointment and even depression from failure to accomplish goals ([[@HeckhausenGollwitzer87]]; [[@OReillyHazyMollickEtAl14]]; [[@OReilly20]]). This framework integrates a wide range of neuroscience, cognitive, and computational data to provide a robust model of how a simulated rodent learns from the outcomes of temporally extended sequences of actions driven by a goal-engaged state. This goal-driven learning provides important error signals that shape learning throughout the network, going beyond what is possible with basic predictive error-driven learning.

At a computational level, the Rubicon + Axon system has the potential to resolve many of the challenging problems associated with [[reinforcement learning#model-based]] reinforcement learning, which all center around the [[curse of dimensionality]] associated with trying to [[search]] through high-dimensional spaces for appropriate goals and action plans. Because Axon performs parallel constraint-satisfaction search at each timestep, based on pervasive bidirectional connectivity, the resulting [[optimized representations]] automatically encode the environment in a way that is strongly constrained by the current goals and plans, allowing novel affordances and problem solutions to emerge.

This dynamic is essentially what we experience subjectively in our [[conscious awareness]], and our analysis suggests that this is not merely an epiphenomenon, but rather an essential reflection of a system that has pervasive access to the contents of its own state of knowledge at every moment, and can use all of these constraints from all over the brain to shape the flow of "thought" to solve problems in ways that purely feedforward architectures cannot.

## From rodent to human

After the rodent level model is fully developed, the next steps involve scaling up to the human level, progressively via intermediate primate-level models, such as that of the widely-studied macaque monkey. Macaques have highly-developed visual perceptual pathways and saccadic motor control networks that provide an active, dynamic modality for constructing increasingly rich internal models of the world. The [[deepvision simulation]] predictive learning model provides an initial attempt to model this system.

In addition to the ability to form and mentally manipulate much richer and more dynamic mental models of the world, the human brain can leverage the incredible power of [[language]], which is widely recognized as an essential ingredient for advanced thought and reasoning abilities. The success of LLMs in developing remarkably capable internal representations of the world through predictive learning of language clearly demonstrates this point, and the Axon / Rubicon framework would add a strong goal-driven motivational system and all of the above-noted abilities to consciously access its own state of knowledge, potentially resolving the longstanding issues with confabulation.

Computationally, the combination of language and a strong goal-driven planning system enables the human brain to function like a [[self-programmable]] [[Turing machine]] that can organize its own behavior in highly flexible, systematic ways. In short, we suggest that the key to full human-level AGI is having this serial, highly flexible, universal form of computation riding on top of a fully bidirectionally connected parallel processing foundation that automatically generates optimized representations through constraint satisfaction, all driven by a strong goal-driven motivational system that keeps everything on track and focused on the most relevant issues.

<!--- TODO: turing with a twist citations! -->

There will be many practical challenges associated with scaling up these models, given their additional biological realism and complexity relative to existing more abstract models. Furthermore, the capacity to actually accomplish significant levels of general problem solving ability in humans requires years of learning and education, representing the apex of the developmental stages mapped out by [[@^Piaget41]]. Thus, this remains a major challenge, but the current objective is to establish a solid biological, computational, and cognitive foundation to understand the essential ingredients of what makes humans so smart.

## Summary

We can summarize the overall approach by way of answering several key questions one must ask in evaluating a theoretical and computational model of the human brain:

1. _Is it scientifically accurate?_ The Axon mechanisms are all grounded in detailed [[neuroscience]] data, and produce known [[cognitive]] phenomena accurately. There are no significant errors of commission in the mechanisms included: each such mechanism has solid evidence in support of it, including critically the basis for powerful error-driven learning via a [[temporal derivative]] computed by the competition between a faster LTP-promoting CaMKII pathway and a slower opposing LTD-promoting DAPK1 pathway in the [[kinase algorithm]]. An initial experimental test of this hypothesis ([[Jiang et al 2025]]) shows consistent evidence.

2. _Does it have a clear principled basis for effective computation?_ The principle of [[search]] through high-dimensional spaces unifies our understanding of both learning and online computation through [[optimized representations]] (via [[constraint satisfaction]] supported by [[bidirectional connectivity]]). The principal challenge is the [[curse of dimensionality]], which requires _parallel_ search mechanisms. Most of the existing [[reinforcement learning#model-based]] reinforcement learning mechanisms do not scale well because they involve serial search of one form or another. By contrast, the [[Rubicon]] framework leverages the parallel mechanisms in Axon, along with a neuroscience-based [[computational-cognitive-neuroscience#reverse engineering]] of the results of millions of years of parallel evolutionary search to build in stronger [[bias-variance tradeoff|biases]] that shape and constrain goal-driven learning.

    From a cognitive perspective, the most direct, intuitive basis for optimism about the overall approach is the connection to [[conscious awareness]] and its grounding in widespread bidirectional interactions among brain areas ([[@Lamme06]]), which suggests that this system could actually _think_ and _reason_ in the same kind of conscious, deliberative manner that we intuitively understand to be taking place in the human brain. Critically, this system should be able to directly access its own state of knowledge in a way that existing models cannot, and use this to direct further reasoning and problem-solving steps via goal-driven cognitive control mechanisms.

3. _Does it actually work in practice, at least at smaller scale?_ Extensive small-scale [[simulations]] demonstrate that the implemented models do work in practice, including demonstrating significant benefits from the spiking-based activations in Axon relative to the earlier [[rate-code activation]] in Leabra and most [[abstract neural network]] models. 

4. _Does it scale effectively?_ The considerable additional computation involved in simulating the constraint satisfaction process via bidirectional excitatory connectivity on each trial of processing, along with the biologically-detailed neural activity functions, represents a significant challenge for scaling up this approach. Thus, the necessary strategy is to establish as strong of a foundation in answering the first 3 questions, to the point that it is then possible to invest the resources necessary to take on this final, definitive step. Existing efforts have at least put the entire computation on parallel GPU hardware in a highly efficient manner, so some of the necessary infrastructure is in place, but much more work remains.

## Axon pages

