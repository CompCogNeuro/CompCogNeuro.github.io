+++
Categories = ["Learning", "Rubicon"]
bibfile = "ccnlab.json"
+++

**Axon** is an attempt to build a biologically accurate, yet still abstracted and computationally tractable model of neural computation in the mammalian brain (e.g., in rodents, monkeys, and people). The goal is to commit as few _errors of commission_ as possible: the features of the model should not violate any known, well-established properties of the brain. There is still plenty of room for _errors of omission_: not every detail can or should be included.

The approach is guided by an in-depth consideration of the available neuroscience literature, as well as a strong consideration for the functional and computational [[levels of analysis]], informed by an understanding of the known cognitive and behavioral capabilities of the respective organisms. In other words, it is [[Computational Cognitive Neuroscience]], leveraging notable [[synergies]] across these different levels of analysis. These synergies provide some measure of confidence that the overall approach is on the right track toward answering the fundamental question: _how do the detailed biological mechanisms of the brain give rise to the computationally sophisticated cognitive abilities of the various mammalian species, including humans_?

Given the considerable scope of neuroscience, cognition, and computation covered here, all from coherent, self-consistent perspective, this material is suitable as an introductory textbook, provided here: [[CCN Intro]]. This textbook treatment builds on a series of prior such [[book|textbooks]] going back to [[@^OReillyMunakata00]], which were based on the [[Leabra]] framework that Axon is the direct successor to. Axon uses more realistic discrete spiking dynamics and associated learning mechanisms relative to Leabra, and supports more advanced brain area models to implement a functional version of the [[Rubicon]] goal-driven learning system, based on the interactions of a large number of [[limbic system|limbic]] brain areas. This motivational system is critical for enabling autonomous learning of cognitive and behavioral skills in an ecologically-realistic manner.

A fundamental challenge for any theoretical understanding of something as complex as the brain, is that we suffer from significant _confirmation bias_ that causes us to contort the scientific data into a form that is consistent with our models, but is not actually how it works. Indeed, the [[bidirectional connectivity]] of the brain that is so central to our models contributes significantly to this phenomenon: it enables us to see what we want to see, not what is actually there. Indeed, we will levy this critique against several other alternative hypotheses along the way (e.g., see [[Hebbian learning]]).

In this context, a primary goal in generating the extensive content here on [CompCogNeuro.org](https://CompCogNeuro.org) is to document the full extent to which the current version of Axon captures the existing scientific findings in neuroscience and cognitive psychology across many [[levels of analysis]], so that the interested reader may form their own opinion about the extent to which the model truly provides an accurate picture of brain function. Feedback on any and all such issues is encouraged, using the [github discussions](https://github.com/CompCogNeuro/CompCogNeuro.github.io/discussions) forum. The [github issues](https://github.com/CompCogNeuro/CompCogNeuro.github.io/issues) can be used to report typos or other such "bugs".

Furthermore, the definitive step in the scientific method is through direct empirical tests of the specific predictions from the model, of which there have been a large number over the years, as documented in the relevant places herein. Perhaps the most central such test is reported in [[Jiang et al 2025]], which directly tests the [[temporal derivative]] form of [[synaptic plasticity]] that drives learning in the Axon model.

## Overview

These are the central elements of Axon in terms of general neural mechanisms, most of which are well established properties of the mammalian [[neocortex]]:

* [[neuron|Spiking neurons]] with relatively long-lasting [[neuron channels#NMDA]] and [[neuron channels#GABA-B]] channels that support [[stable activation]] states over the course of a roughly 200 msec [[theta cycle]], which is essential for establishing a coherent representation of the current input state. This stability is necessary to drive effective learning as described next. The Axon neuron model is entirely conventional from a computational neuroscience perspective, featuring two compartments (soma and [[neuron dendrites|dendrite]]) using a range of conductance-based electrophysiologically-accurate [[neuron channels|channels]] and the widely-used _AdEx_ adaptive exponential approximation to the full Hodgkin-Huxley spiking dynamics ([[@BretteGerstner05]]).

    The discrete spiking behavior of these neurons enables effective graded information integration over time in a way that continuous [[rate code activation]] communication does not, by allowing many different signals to be communicated over time, competing for the overall control of the network activation state as a function of the collective integration of spikes within the neurons in the network. As a result, Axon models are overall much more robust and well-behaved overall compared to their [[Leabra]] rate-code based counterparts.

* [[Error-driven learning]] based on errors computed via [[temporal derivative]]s that naturally supports [[predictive learning]] in terms of the difference over time of network states representing the prediction followed by the outcome. Local [[synaptic plasticity]] based on the competition between kinases updating at different rates, i.e., the [[kinase algorithm]], naturally computes the error gradient via the temporal derivative dynamic, supporting a fully biologically plausible form of the computationally powerful [[error backpropagation]] algorithm. Initial empirical support for this mechanism is reported in [[Jiang et al 2025]].

<!--- todo: indent and + sub-items are not working -->

The combination of robust error-driven learning and biologically-detailed spiking neurons in Axon enables these neurons to learn to perform arbitrary computational and cognitive tasks. Furthermore, the availability of a clear computational measure of performance in terms of overall learning capability across a wide range of tasks has enabled the optimization of all the biological parameters to maximize learning performance. There is a consistent set of such parameters that generally works best across all the tasks investigated to date, and thus the additional degrees of freedom associated with these parameters are generally eliminated from consideration in constructing new models, greatly reducing the effective complexity of the model.

* [[Bidirectional connectivity]] among excitatory neurons, which is necessary for propagating error signals throughout the network, and pooled [[inhibition]] which is necessary for controlling the effects of bidirectional excitatory connectivity, while also having beneficial computational effects in terms of [[attention]] and [[competition]]. Bidirectional connectivity also supports multiple [[constraint satisfaction]] dynamics that can efficiently search through large high-dimensional knowledge spaces to find (and synthesize) the most relevant information given the current bottom-up (sensory) and top-down (goals) constraints.

    Perhaps most importantly, this bidirectional connectivity is widely thought to be essential for [[conscious awareness]] ([[@Lamme06]]), which is likely critical for the system to access its own internal state of knowledge. This ability is notably absent in current [[abstract neural network]] models that drive the widely-used [[large language model]]s for example, which are notorious for their inability to accurately evaluate their own knowledge states, resulting in significant _confabulation_. In addition, most experts do not think these models are conscious. Notably, these models are based exclusively on [[feedforward connectivity]], consistent with the idea that bidirectional connectivity is essential for consciousness, and the functional benefits associated with it.

The central role of bidirectional connectivity in Axon represents one of the most important points of divergence relative to the vast majority of existing neural network models (along with the combination of biologically-detailed spiking dynamics and error-driven learning), and testing the functional importance of this property is a major overarching goal of this research.

These basic neural mechanisms are sufficient to learn well-established functions of the [[posterior neocortex]], including spatially invariant [[object recognition]]. However, another critical hypothesis for the Axon framework is that successful learning and performance in complex real-world environments requires significant additional neural circuits and systems to support _goal-driven_ learning and performance across longer time scales in a coordinated manner, which is captured in the [[Rubicon]] framework.

The goal-driven system must be sensitive to the overall costs and benefits of actions, in order to commit time and effort to a consistent plan of behavior to accomplish a given goal. Thus, the brain systems in the Rubicon model include various subcortical areas involved in representing reward, punishment, and effort, based on the detailed biology of the [[basal ganglia]], [[amygdala]], [[dopamine]] and [[acetylcholine]], along with higher-level [[prefrontal cortex]] and [[hippocampus]] areas. These areas are traditionally known as the [[limbic system]], and constitute a significant fraction (roughly 1/3) of the brain mass in a the rat brain. Overall, this goal-driven learning provides critical error signals that shape learning throughout the network, going beyond what is possible with basic predictive error-driven learning.

## Axon pages

