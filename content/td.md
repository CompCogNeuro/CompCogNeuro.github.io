+++
Categories = ["Reinforcement learning", "Rubicon"]
bibfile = "ccnlab.json"
+++

The **temporal differences (TD)** algorithm is one of the most central abstract learning rules used in [[reinforcement learning]] ([[@Samuel59]]; [[@SuttonBarto81]]; [[@Sutton88]]; [[@SuttonBarto98]]), and it provides a remarkably accurate explanation for many of the firing properties of [[dopamine]] neurons in the ventral tegmental area (VTA) of the brainstem ([[@MontagueDayanSejnowski96]]; [[@SchultzDayanMontague97]]).

Specifically TD provides a way of computing the [[#reward prediction error]] (RPE) signal in terms of the difference between predicted and actual reward outcomes. In the [[actor critic]] architecture ([[@BartoSuttonAnderson83]]), this RPE signal is generated by the _critic_, which provides the training signal for improving both motor actions taken by the _actor_ to obtain reward, and the accuracy of the critic's reward predictions. This is consistent with considerable neuroscience data showing that dopamine modulates learning throughout the brain, especially in the [[basal ganglia]] which has both actor and critic functionality.

Although they share some core conceptual similarities, and a very similar name, it is important to distinguish the TD framework from the [[temporal derivative]] based learning mechanisms used in [[Axon]] for [[error-driven learning]] in the [[neocortex]], via the [[kinase algorithm]]. Most importantly, from a biological perspective, TD translates a temporal difference signal into the firing of dopamine neurons, which then _explicitly_ represents the RPE signal. By contrast, the prediction error signal in the temporal derivative framework _remains implicit_ as a difference in neural activity over time, which propagates throughout the neocortex via bidirectional connectivity. Each synapse then adjusts its synaptic weights locally in a way that is sensitive to these temporal derivatives, as contrasted with the direct neuromodulatory role played by dopamine.

From a neuroscience perspective there are actually a number of different brain areas and circuits that work together to drive the firing of VTA dopamine neurons, as summarized in the [[PVLV]] model ([[@OReillyFrankHazyEtAl07]]; [[@HazyFrankOReilly10]]; [[@MollickHazyKruegerEtAl20]]). These mechanisms produce some important differences in dopamine firing behavior relative to the predictions of the TD model, including critically the absence of a chain-like progression of firing at the time of the US to the time of the CS, among others ([[@MollickHazyKruegerEtAl20]]). Nevertheless, the TD model provides an elegant and computationally powerful learning framework that explains many critical aspects of the role of dopamine in learning across different brain areas.

## Reward prediction error

The RPE is the main computational result of the TD algorithm:

TODO:

## Mathematical derivation

TODO:

